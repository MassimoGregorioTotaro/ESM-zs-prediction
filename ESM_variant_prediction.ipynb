{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassimoGregorioTotaro/ESM-zs-prediction/blob/main/ESM_variant_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ESM zero-shot variant prediction**\n",
        "for more details see: [Github](https://github.com/facebookresearch/esm/tree/main/esm), [Paper](https://doi.org/10.1101/2021.07.09.450648)\n",
        "\n",
        "#### **Tips and Instructions**\n",
        "- click the little â–¶ play icon to the left of each cell below.\n",
        "- once the install and module setup cells are run, you shouldn't run them again.\n",
        "- there's three running modes that can be chosen in the predict cell:\n",
        "  - 'seq vs seq' which expects a sequence, similar to the input one, with corresponding aminoacid substitutions which will be evaluated;\n",
        "  - 'deep mutational scan' which expects a list of residue indexes where the algorithm weill try and place all the other 19 aminoacid substituents and estimate a score value for the individual substitution (e.g. 1 100);\n",
        "  - 'aa substitutions' where the mutations to be evaluated have to be written down explicitly (e.g. M1W D100R);\n"
      ],
      "metadata": {
        "id": "POQBeXf2Xoxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install (run only once per session)\n",
        "\n",
        "# Copyright (c) 2023, Massimo G. Totaro All rights reserved.\n",
        "# Redistribution and use in source and binary forms, with or without \n",
        "# modification, are permitted provided that the following conditions are met:\n",
        "# 1. Redistributions of source code must retain the above copyright notice, this\n",
        "#    list of conditions and the following disclaimer.\n",
        "# 2. Redistributions in binary form must reproduce the above copyright notice, \n",
        "#    this list of conditions and the following disclaimer in the documentation \n",
        "#    and/or other materials provided with the distribution.\n",
        "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \n",
        "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n",
        "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE \n",
        "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "!pip install -q biopython fair-esm torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kqx5U8JeLdh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title model setup and training (run only once per session; modify only if you know what you're doing)\n",
        "import string\n",
        "\n",
        "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def remove_insertions(sequence: str) -> str:\n",
        "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
        "    # This is an efficient way to delete lowercase characters and insertion characters from a string\n",
        "    deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
        "    deletekeys[\".\"] = None\n",
        "    deletekeys[\"*\"] = None\n",
        "\n",
        "    translation = str.maketrans(deletekeys)\n",
        "    return sequence.translate(translation)\n",
        "\n",
        "\n",
        "def label_row(row, sequence, token_probs, alphabet, offset_idx):\n",
        "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
        "    assert sequence[idx] == wt, \"The listed wildtype does not match the provided sequence\"\n",
        "\n",
        "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
        "\n",
        "    # add 1 for BOS\n",
        "    score = token_probs[0, 1 + idx, mt_encoded] - token_probs[0, 1 + idx, wt_encoded]\n",
        "    return score.item()\n",
        "\n",
        "\n",
        "def compute_pppl(row, sequence, model, alphabet, offset_idx):\n",
        "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
        "    assert sequence[idx] == wt, \"The listed wildtype does not match the provided sequence\"\n",
        "\n",
        "    # modify the sequence\n",
        "    sequence = sequence[:idx] + mt + sequence[(idx + 1) :]\n",
        "\n",
        "    # encode the sequence\n",
        "    data = [\n",
        "        (\"protein1\", sequence),\n",
        "    ]\n",
        "\n",
        "    batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
        "\n",
        "    # compute probabilities at each position\n",
        "    log_probs = []\n",
        "    for i in range(1, len(sequence) - 1):\n",
        "        batch_tokens_masked = batch_tokens.clone()\n",
        "        batch_tokens_masked[0, i] = alphabet.mask_idx\n",
        "        with torch.no_grad():\n",
        "            token_probs = torch.log_softmax(model(batch_tokens_masked.cuda())[\"logits\"], dim=-1)\n",
        "        log_probs.append(token_probs[0, i, alphabet.get_idx(sequence[i])].item())  # vocab size\n",
        "    return sum(log_probs)\n",
        "\n",
        "model_name = 'esm2_t33_650M_UR50D' #@param ['esm1v_t33_650M_UR90S_1', 'esm1v_t33_650M_UR90S_2', 'esm1v_t33_650M_UR90S_3', 'esm1v_t33_650M_UR90S_4', 'esm1v_t33_650M_UR90S_5', 'esm2_t33_650M_UR50D', 'esm2_t36_3B_UR50D', 'esm2_t48_15B_UR50D']\n",
        "\n",
        "# model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", model_name) ## esm update broke this, well done, guys!\n",
        "model, alphabet = eval(f\"pretrained.{model_name}()\")\n",
        "model.eval()\n",
        "scoring_strategy = \"wt-marginals\" #@param [\"wt-marginals\", \"pseudo-ppl\", \"masked-marginals\"]\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(\"Transferred model to GPU\")"
      ],
      "metadata": {
        "id": "yynOMVaeiT5U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title predict\n",
        "\n",
        "mode = 'deep mutational scan'  #@param ['seq vs seq', 'deep mutational scan', 'aa substitutions']\n",
        "sequence = \"\" #@param {type:\"string\"}\n",
        "target = \"\" #@param {type:\"string\"}\n",
        "offset_idx = 1 #\\@param {type:\"integer\"}\n",
        "substitutions = list()\n",
        "\n",
        "if mode == 'seq vs seq':\n",
        "  for resi,(src,trg) in enumerate(zip(sequence,target), offset_idx):\n",
        "    if src != trg:\n",
        "      substitutions.append(f\"{src}{resi}{trg}\")\n",
        "elif mode == 'deep mutational scan':\n",
        "  for resi in map(int, target.split()):\n",
        "    src = sequence[resi-offset_idx]\n",
        "    for trg in \"ACDEFGHIKLMNPQRSTVWY\".replace(src,''):\n",
        "      substitutions.append(f\"{src}{resi}{trg}\")\n",
        "elif mode == 'aa substitutions':\n",
        "  substitutions = target.split()\n",
        "else:\n",
        "  raise RuntimeError(\"Unrecognised running mode\")\n",
        "\n",
        "df = pd.DataFrame(substitutions, columns=['\\0'])\n",
        "mutation_col = df.columns[0]\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "data = [(\"protein1\", sequence),]\n",
        "\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "if scoring_strategy == \"wt-marginals\":\n",
        "    with torch.no_grad():\n",
        "        token_probs = torch.log_softmax(model(batch_tokens.cuda())[\"logits\"], dim=-1)\n",
        "    df[model_name] = df.apply(\n",
        "        lambda row: label_row(\n",
        "            row[mutation_col],\n",
        "            sequence,\n",
        "            token_probs,\n",
        "            alphabet,\n",
        "            offset_idx,\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "elif scoring_strategy == \"masked-marginals\":\n",
        "    all_token_probs = []\n",
        "    for i in tqdm(range(batch_tokens.size(1))):\n",
        "        batch_tokens_masked = batch_tokens.clone()\n",
        "        batch_tokens_masked[0, i] = alphabet.mask_idx\n",
        "        with torch.no_grad():\n",
        "            token_probs = torch.log_softmax(\n",
        "                model(batch_tokens_masked.cuda())[\"logits\"], dim=-1\n",
        "            )\n",
        "        all_token_probs.append(token_probs[:, i])  # vocab size\n",
        "    token_probs = torch.cat(all_token_probs, dim=0).unsqueeze(0)\n",
        "    df[model_name] = df.apply(\n",
        "        lambda row: label_row(\n",
        "            row[mutation_col],\n",
        "            sequence,\n",
        "            token_probs,\n",
        "            alphabet,\n",
        "            offset_idx,\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "elif scoring_strategy == \"pseudo-ppl\":\n",
        "    tqdm.pandas()\n",
        "    df[model_name] = df.progress_apply(\n",
        "        lambda row: compute_pppl(\n",
        "            row[mutation_col], sequence, model, alphabet, offset_idx\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "if mode == 'aa substitutions':\n",
        "  df = df.sort_values(model_name, ascending=False)\n",
        "elif mode == 'deep mutational scan':\n",
        "  df = pd.concat([(df.assign(resi=df['\\0'].str.extract(f'(\\d+)', expand=False).astype(int))\n",
        "          .sort_values(['resi', model_name], ascending=[True,False])\n",
        "          .groupby(['resi'])\n",
        "          .head(19)\n",
        "          .drop(['resi'], axis=1)).iloc[19*x:19*(x+1)].reset_index(drop=True) for x in range(df.shape[0]//19)]\n",
        "        , axis=1).set_axis(range(df.shape[0]//19*2), axis='columns')\n",
        "\n",
        "df.style.hide_index().hide_columns().background_gradient(cmap=\"RdYlGn\", vmax=8, vmin=-8)"
      ],
      "metadata": {
        "id": "jAY2WEyYMNCG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}